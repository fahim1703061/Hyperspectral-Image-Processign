{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNDqzpIs/sxBZggGUWcBf10",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fahim1703061/Hyperspectral-Image-Processing/blob/main/Research_data_preprocess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/ringochuchudull/CNN-Based-Hyperspectral-Image-Classification"
      ],
      "metadata": {
        "id": "mgVftKZbDq6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getdtype(t):\n",
        "    import numpy as np\n",
        "    if t == 'float64':\n",
        "        return np.float64\n",
        "    elif t == 'float32':\n",
        "        return np.float32\n",
        "    elif t == 'float16':\n",
        "        return np.float16\n",
        "    elif t == 'int64':\n",
        "        return np.int64\n",
        "    elif t == 'int32':\n",
        "        return np.int32\n",
        "    elif t == 'int16':\n",
        "        return np.int16\n",
        "    elif t == 'int8':\n",
        "        return np.int8\n",
        "    else:\n",
        "        # Default value\n",
        "        return np.float64\n",
        "\n",
        "#Get Dataset\n",
        "def maybeExtract(data, patch_size):\n",
        "    import scipy.io\n",
        "    try:\n",
        "        TRAIN = scipy.io.loadmat(\"./data/\" + data + \"_Train_patch_\" + str(patch_size) + \".mat\")\n",
        "        VALIDATION = scipy.io.loadmat(\"./data/\" + data + \"_Val_patch_\" + str(patch_size) + \".mat\")\n",
        "        TEST = scipy.io.loadmat(\"./data/\" + data + \"_Test_patch_\" + str(patch_size) + \".mat\")\n",
        "\n",
        "    except:\n",
        "        raise Exception('--data options are: Indian_pines, Salinas, KSC, Botswana OR data files not existed')\n",
        "\n",
        "    return TRAIN, VALIDATION, TEST\n",
        "\n",
        "\n",
        "def maybeDownloadOrExtract(data):\n",
        "    import scipy.io as io\n",
        "    import os\n",
        "    # Somehow this is necessary, even I cannot tell why -_-\n",
        "    if data in ('KSC', 'Botswana'):\n",
        "        filename = data\n",
        "    else:\n",
        "        filename = data.lower()\n",
        "\n",
        "    print(\"Dataset: \" + filename)\n",
        "\n",
        "    try:\n",
        "        print(\"Try using images from Data folder...\")\n",
        "        input_mat = io.loadmat('./data/' + data + '.mat')[filename]\n",
        "        target_mat = io.loadmat('./data/' + data + '_gt.mat')[filename + '_gt']\n",
        "\n",
        "    except:\n",
        "        print(\"Data not found, downloading input images and labelled images!\\n\\n\")\n",
        "        if data == \"Indian_pines\":\n",
        "            url1 = \"http://www.ehu.eus/ccwintco/uploads/2/22/Indian_pines.mat\"\n",
        "            url2 = \"http://www.ehu.eus/ccwintco/uploads/c/c4/Indian_pines_gt.mat\"\n",
        "\n",
        "        elif data == \"Salinas\":\n",
        "            url1 = \"http://www.ehu.eus/ccwintco/uploads/f/f1/Salinas.mat\"\n",
        "            url2 = \"http://www.ehu.eus/ccwintco/uploads/f/fa/Salinas_gt.mat\"\n",
        "\n",
        "        elif data == \"KSC\":\n",
        "            url1 = \"http://www.ehu.eus/ccwintco/uploads/2/26/KSC.mat\"\n",
        "            url2 = \"http://www.ehu.eus/ccwintco/uploads/a/a6/KSC_gt.mat\"\n",
        "\n",
        "        elif data == \"Botswana\":\n",
        "            url1 = \"http://www.ehu.eus/ccwintco/uploads/7/72/Botswana.mat\"\n",
        "            url2 = \"http://www.ehu.eus/ccwintco/uploads/5/58/Botswana_gt.mat\"\n",
        "\n",
        "        else:\n",
        "            raise Exception(\"Available datasets are:: Indian_pines, Salinas, KSC, Botswana\")\n",
        "\n",
        "        os.system('wget -P' + ' ' + './data/' + ' ' + url1)\n",
        "        os.system('wget -P' + ' ' + './data/' + ' ' + url2)\n",
        "\n",
        "        input_mat = io.loadmat('./data/' + data + '.mat')[filename]\n",
        "        target_mat = io.loadmat('./data/' + data + '_gt.mat')[filename + '_gt']\n",
        "\n",
        "    return input_mat, target_mat\n",
        "\n",
        "\n",
        "def getListLabel(data):\n",
        "    if data == 'Indian_pines':\n",
        "        return [2, 3, 4, 5, 6, 8, 10, 11, 12, 14, 15]\n",
        "\n",
        "    elif data == 'Salinas':\n",
        "        return list(range(1,16+1))\n",
        "\n",
        "    elif data == 'Botswana':\n",
        "        return [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,  13, 14]\n",
        "\n",
        "    elif data == 'KSC':\n",
        "        return [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
        "\n",
        "    else:\n",
        "        raise Exception(\"Type error\")\n",
        "\n",
        "\n",
        "\n",
        "def OnehotTransform(labels):\n",
        "    import numpy as np\n",
        "    from sklearn.preprocessing import OneHotEncoder\n",
        "    onehot_encoder = OneHotEncoder(sparse=False)\n",
        "\n",
        "    labels = np.reshape(labels, (len(labels), 1))\n",
        "    labels = onehot_encoder.fit_transform(labels).astype(np.uint8)\n",
        "\n",
        "    return labels\n",
        "\n",
        "\n",
        "\n",
        "def getTestDataset(test, test_label, size=250):\n",
        "    '''\n",
        "    Arguments: whole test data, test label,\n",
        "    return randomized test data, test label of 'size'\n",
        "    '''\n",
        "    from numpy import array\n",
        "    from random import shuffle\n",
        "\n",
        "    assert test.shape[0] == test_label.shape[0]\n",
        "\n",
        "    idx = list(range(test.shape[0]))\n",
        "    shuffle(idx)\n",
        "    idx = idx[:size]\n",
        "    accuracy_x, accuracy_y = [], []\n",
        "    for i in idx:\n",
        "        accuracy_x.append(test[i])\n",
        "        accuracy_y.append(test_label[i])\n",
        "\n",
        "    return array(accuracy_x), array(accuracy_y)\n",
        "\n",
        "\n",
        "def plot_random_spec_img(pic, true_label):\n",
        "    '''\n",
        "    Take first hyperspectral image from dataset and plot spectral data distribution\n",
        "    Arguements pic = list of images in size (?, height, width, bands), where ? represents any number > 0\n",
        "                true_labels = lists of ground truth corrospond to pic\n",
        "    '''\n",
        "    pic = pic[0]  #Take first data only\n",
        "    from matplotlib import pyplot as plt\n",
        "    from mpl_toolkits.mplot3d import Axes3D\n",
        "    from numpy import mean, argmax\n",
        "\n",
        "    print(\"Image Shape: \" + str(pic.shape) )\n",
        "    print(\"Label of this image is -> \" + str(true_label[0] ) )\n",
        "\n",
        "    title = argmax(true_label[0], axis=0)\n",
        "    # Calculate mean of all elements in the 3d element\n",
        "    mean_value = mean(pic)\n",
        "    # Replace element with less than mean by zero\n",
        "    pic[pic < mean_value] = 0\n",
        "    \n",
        "    x = []\n",
        "    y = []\n",
        "    z = []\n",
        "    # Coordinate position extractions\n",
        "    for z1 in range(pic.shape[0]): \n",
        "        for x1 in range(pic.shape[1]):\n",
        "            for y1 in range(pic.shape[2]):\n",
        "                if pic[z1,x1,y1] != 0:\n",
        "                    z.append(z1)\n",
        "                    x.append(x1)\n",
        "                    y.append(y1)\n",
        "\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    ax.set_title('True class = '+ str(title))\n",
        "    ax.scatter(x, y, z, color='#0606aa', marker='o', s=0.5)\n",
        "    ax.set_xlabel('X Label')\n",
        "    ax.set_ylabel('Spectral Label')\n",
        "    ax.set_zlabel('Y Label')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def GroundTruthVisualise(data):\n",
        "    from matplotlib.pyplot import imshow, show, colorbar, set_cmap\n",
        "    imshow(data)\n",
        "    set_cmap('tab20b')\n",
        "    colorbar()\n",
        "    show()\n",
        "\n",
        "\n",
        "# Arguement: data = 3D image in size (h,w,bands)\n",
        "def plotStatlieImage(data, bird=False):\n",
        "    from matplotlib.pyplot import imshow, show, subplots, axis, figure\n",
        "    print('\\nPlotting a band image')\n",
        "    fig, ax = subplots(nrows=3, ncols=3)\n",
        "    i = 1\n",
        "    for row in ax:\n",
        "        for col in row:\n",
        "            i += 11\n",
        "            if bird:\n",
        "                col.imshow(data[i,:,:])\n",
        "            else:\n",
        "                col.imshow(data[:,:,i])\n",
        "            axis('off')\n",
        "    show()\n",
        "\n",
        "\n",
        "def showClassTable(number_of_list, title='Number of samples'):\n",
        "    import pandas as pd \n",
        "    print(\"\\n+------------Show Table---------------+\")\n",
        "    lenth = len(number_of_list)\n",
        "    column1 = range(1, lenth+1)\n",
        "    table = {'Class#': column1, title: number_of_list}\n",
        "    table_df = pd.DataFrame(table).to_string(index=False)\n",
        "    print(table_df)   \n",
        "    print(\"+-----------Close Table---------------+\")\n",
        "\n",
        "\n",
        "\n",
        "# This section here is for debugs only\n",
        "if __name__ == '__main__':\n",
        "    pass"
      ],
      "metadata": {
        "id": "2H1r2UF-C7LJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mjvgs5C5owy",
        "outputId": "9f32fcd3-e4a2-4bdb-e9c6-8a64379afb9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: indian_pines\n",
            "Try using images from Data folder...\n",
            "Data not found, downloading input images and labelled images!\n",
            "\n",
            "\n",
            "+-------------------------------------+\n",
            "Input_mat shape: (145, 145, 220)\n",
            "Extracting Indian_pines dataset features.../Total excution time...13.840296745300293seconds\n",
            "Total number of samples: 10249\n",
            "Finished!\t\n",
            "+------------Show Table---------------+\n",
            " Class#  Number of samples\n",
            "      1                 46\n",
            "      2               1428\n",
            "      3                830\n",
            "      4                237\n",
            "      5                483\n",
            "      6                730\n",
            "      7                 28\n",
            "      8                478\n",
            "      9                 20\n",
            "     10                972\n",
            "     11               2455\n",
            "     12                593\n",
            "     13                205\n",
            "     14               1265\n",
            "     15                386\n",
            "     16                 93\n",
            "+-----------Close Table---------------+\n",
            "-Class 1 is rejected due to insufficient samples\n",
            "Class 2 is accepted\n",
            "Class 3 is accepted\n",
            "Class 4 is accepted\n",
            "Class 5 is accepted\n",
            "Class 6 is accepted\n",
            "-Class 7 is rejected due to insufficient samples\n",
            "Class 8 is accepted\n",
            "-Class 9 is rejected due to insufficient samples\n",
            "Class 10 is accepted\n",
            "Class 11 is accepted\n",
            "Class 12 is accepted\n",
            "-Class 13 is rejected due to insufficient samples\n",
            "Class 14 is accepted\n",
            "Class 15 is accepted\n",
            "-Class 16 is rejected due to insufficient samples\n",
            "+-------------------------------------+\n",
            "Size of Training data: 1972\n",
            "Size of Validation data: 493\n",
            "Size of Testing data: 7392\n",
            "+-------------------------------------+\n",
            "(493, 5, 5, 220)\n",
            "+-------------------------------------+\n",
            "Summary\n",
            "Train_patch.shape: (1972, 5, 5, 220)\n",
            "Train_label.shape: (1972, 11)\n",
            "Test_patch.shape: (7392, 5, 5, 220)\n",
            "Test_label.shape: (7392, 11)\n",
            "Validation batch Shape: (493, 5, 5, 220)\n",
            "Validation label Shape: (493, 11)\n",
            "+-------------------------------------+\n",
            "\n",
            "Finished processing.......\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from random import shuffle\n",
        "import scipy.io as io\n",
        "import argparse\n",
        "from helper import *\n",
        "import threading\n",
        "import time\n",
        "import itertools\n",
        "import sys\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('-f')\n",
        "parser.add_argument('--data', type=str, default='Indian_pines', help='Default: Indian_pines, options: Salinas, KSC, Botswana')\n",
        "parser.add_argument('--train_ratio', type=float, default=0.2)\n",
        "parser.add_argument('--validation_ratio', type=float, default=0.05)\n",
        "parser.add_argument('--channel_first', type=bool, default=False, help='Image channel located on the last dimension')\n",
        "parser.add_argument('--dtype', type=str, default='float32', help='Data type (Eg float64, float32, float16, int64...')\n",
        "parser.add_argument('--plot', type=bool, default=False, help='TRUE to plot satellite images and ground truth at the end')\n",
        "opt = parser.parse_args()\n",
        "\n",
        "# Try loading data from the folder... Otherwise download from online\n",
        "input_mat, target_mat = maybeDownloadOrExtract(opt.data)\n",
        "\n",
        "# Output data type\n",
        "datatype = getdtype(opt.dtype)\n",
        "HEIGHT = input_mat.shape[0]\n",
        "WIDTH = input_mat.shape[1]\n",
        "BAND = input_mat.shape[2]\n",
        "OUTPUT_CLASSES = np.max(target_mat)\n",
        "PATCH_SIZE = 5\n",
        "\n",
        "CHANNEL_FIRST = opt.channel_first\n",
        "\n",
        "# Normalize image data and select datatype\n",
        "input_mat = input_mat.astype(datatype)\n",
        "input_mat = input_mat - np.min(input_mat)\n",
        "input_mat = input_mat / np.max(input_mat)\n",
        "\n",
        "# Extract a list that contains the class number with sufficient training samples\n",
        "list_labels = getListLabel(opt.data)\n",
        "\n",
        "# For showing a animation only\n",
        "end_loading = False\n",
        "def animate():\n",
        "    global end_loading\n",
        "    for c in itertools.cycle(['|', '/', '-', '\\\\']):\n",
        "        if end_loading:\n",
        "            break\n",
        "        sys.stdout.write('\\rExtracting '+ opt.data + ' dataset features...' + c)\n",
        "        sys.stdout.flush()\n",
        "        time.sleep(0.1)\n",
        "        sys.stdout.write('\\rFinished!\\t')\n",
        "\n",
        "print(\"+-------------------------------------+\")\n",
        "print('Input_mat shape: ' + str(input_mat.shape))\n",
        "\n",
        "MEAN_ARRAY = np.ndarray(shape=(BAND, 1))\n",
        "new_input_mat = []\n",
        "\n",
        "input_mat = np.transpose(input_mat, (2, 0, 1))\n",
        "\n",
        "calib_val_pad = int((PATCH_SIZE - 1)/2)\n",
        "for i in range(BAND):\n",
        "    MEAN_ARRAY[i] = np.mean(input_mat[i, :, :])\n",
        "    new_input_mat.append(np.pad(input_mat[i, :, :], calib_val_pad, 'constant', constant_values=0))\n",
        "\n",
        "input_mat = np.array(new_input_mat)\n",
        "\n",
        "def Patch(height_index, width_index):\n",
        "\n",
        "    # Input:\n",
        "    # Given the index position (x,y) of spatio dimension of the hyperspectral image,\n",
        "\n",
        "    # Output:\n",
        "    # a data cube with patch size S (24 neighbours), with label based on central pixel\n",
        "\n",
        "    height_slice = slice(height_index, height_index+PATCH_SIZE)\n",
        "    width_slice = slice(width_index, width_index+PATCH_SIZE)\n",
        "\n",
        "    patch = input_mat[:, height_slice, width_slice]\n",
        "    mean_normalized_patch = []\n",
        "    for i in range(patch.shape[0]):\n",
        "        mean_normalized_patch.append(patch[i] - MEAN_ARRAY[i])\n",
        "\n",
        "    return np.array(mean_normalized_patch).astype(datatype)\n",
        "\n",
        "\n",
        "# Assign empty array to store patched images\n",
        "CLASSES = []\n",
        "for i in range(OUTPUT_CLASSES):\n",
        "    CLASSES.append([])\n",
        "\n",
        "# Assign empty array to count samples in each class\n",
        "class_label_counter = [0] * OUTPUT_CLASSES\n",
        "\n",
        "# Start timing for loading\n",
        "t = threading.Thread(target=animate).start()\n",
        "start = time.time()\n",
        "\n",
        "count = 0\n",
        "for i in range(HEIGHT-1):\n",
        "    for j in range(WIDTH-1):\n",
        "        curr_inp = Patch(i, j)\n",
        "        curr_tar = target_mat[i, j]\n",
        "\n",
        "        if curr_tar:\n",
        "            CLASSES[curr_tar-1].append(curr_inp)\n",
        "            class_label_counter[curr_tar-1] += 1\n",
        "            count += 1\n",
        "\n",
        "end_loading = True\n",
        "end = time.time()\n",
        "print(\"Total excution time...\" + str(end-start)+'seconds')\n",
        "print('Total number of samples: ' + str(count))\n",
        "showClassTable(class_label_counter)\n",
        "\n",
        "TRAIN_PATCH, TRAIN_LABELS = [], []\n",
        "TEST_PATCH, TEST_LABELS =[], []\n",
        "VAL_PATCH, VAL_LABELS = [], []\n",
        "\n",
        "train_ratio = opt.train_ratio\n",
        "val_ratio = opt.validation_ratio\n",
        "# test_ratio = reminder of data\n",
        "\n",
        "counter = 0  # Represent train_index position\n",
        "for i, data in enumerate(CLASSES):\n",
        "    datasize = []\n",
        "    if i + 1 in list_labels:\n",
        "\n",
        "        shuffle(data)\n",
        "        print('Class ' + str(i + 1) + ' is accepted')\n",
        "\n",
        "        size = round(class_label_counter[i]*train_ratio)\n",
        "\n",
        "        TRAIN_PATCH += data[:size]\n",
        "        TRAIN_LABELS += [counter] * size\n",
        "        datasize.append(size)\n",
        "\n",
        "        size1 = round(class_label_counter[i]*val_ratio)\n",
        "        VAL_PATCH += data[size:size+size1]\n",
        "        VAL_LABELS += [counter] * (size1)\n",
        "        datasize.append(size1)\n",
        "\n",
        "        TEST_PATCH += data[size+size1:]\n",
        "        TEST_LABELS += [counter] * len(data[size+size1:])\n",
        "        datasize.append(len(TEST_PATCH))\n",
        "\n",
        "        counter += 1\n",
        "\n",
        "    else:\n",
        "        print('-Class ' + str(i + 1) + ' is rejected due to insufficient samples')\n",
        "\n",
        "TRAIN_LABELS = np.array(TRAIN_LABELS)\n",
        "TRAIN_PATCH = np.array(TRAIN_PATCH)\n",
        "TEST_PATCH = np.array(TEST_PATCH)\n",
        "TEST_LABELS = np.array(TEST_LABELS)\n",
        "VAL_PATCH = np.array(VAL_PATCH)\n",
        "VAL_LABELS = np.array(VAL_LABELS)\n",
        "\n",
        "print(\"+-------------------------------------+\")\n",
        "print(\"Size of Training data: \" + str(len(TRAIN_PATCH)) )\n",
        "print(\"Size of Validation data: \" + str(len(VAL_PATCH))  )\n",
        "print(\"Size of Testing data: \" + str(len(TEST_PATCH)) )\n",
        "print(\"+-------------------------------------+\")\n",
        "\n",
        "\n",
        "train_idx = list(range(len(TRAIN_PATCH)))\n",
        "shuffle(train_idx)\n",
        "TRAIN_PATCH = TRAIN_PATCH[train_idx]\n",
        "if not CHANNEL_FIRST:\n",
        "    TRAIN_PATCH = np.transpose(TRAIN_PATCH, (0, 2, 3, 1))\n",
        "TRAIN_LABELS = OnehotTransform(TRAIN_LABELS[train_idx])\n",
        "train = {}\n",
        "train[\"train_patch\"] = TRAIN_PATCH\n",
        "train[\"train_labels\"] = TRAIN_LABELS\n",
        "io.savemat(\"./data/\" + opt.data + \"_Train_patch_\" + str(PATCH_SIZE) + \".mat\", train)\n",
        "\n",
        "\n",
        "test_idx = list(range(len(TEST_PATCH)))\n",
        "shuffle(test_idx)\n",
        "TEST_PATCH = TEST_PATCH[test_idx]\n",
        "if not CHANNEL_FIRST:\n",
        "    TEST_PATCH = np.transpose(TEST_PATCH, (0, 2, 3, 1))\n",
        "TEST_LABELS = OnehotTransform(TEST_LABELS[test_idx])\n",
        "test = {}\n",
        "test[\"test_patch\"] = TEST_PATCH\n",
        "test[\"test_labels\"] = TEST_LABELS\n",
        "io.savemat(\"./data/\" + opt.data + \"_Test_patch_\" + str(PATCH_SIZE) + \".mat\", test)\n",
        "\n",
        "\n",
        "val_idx = list(range(len(VAL_PATCH)))\n",
        "shuffle(val_idx)\n",
        "VAL_PATCH = VAL_PATCH[val_idx]\n",
        "if not CHANNEL_FIRST:\n",
        "    VAL_PATCH = np.transpose(VAL_PATCH, (0, 2, 3, 1))\n",
        "    print(VAL_PATCH.shape)\n",
        "VAL_LABELS = OnehotTransform(VAL_LABELS[val_idx])\n",
        "val = {}\n",
        "val[\"val_patch\"] = VAL_PATCH\n",
        "val[\"val_labels\"] = VAL_LABELS\n",
        "io.savemat(\"./data/\" + opt.data + \"_Val_patch_\" + str(PATCH_SIZE) + \".mat\", val)\n",
        "\n",
        "print(\"+-------------------------------------+\")\n",
        "print(\"Summary\")\n",
        "print('Train_patch.shape: '+ str(TRAIN_PATCH.shape) )\n",
        "print('Train_label.shape: '+ str(TRAIN_LABELS.shape) )\n",
        "print('Test_patch.shape: ' + str(TEST_PATCH.shape))\n",
        "print('Test_label.shape: ' + str(TEST_LABELS.shape))\n",
        "print(\"Validation batch Shape: \" + str(VAL_PATCH.shape) )\n",
        "print(\"Validation label Shape: \" + str(VAL_LABELS.shape) )\n",
        "print(\"+-------------------------------------+\")\n",
        "print(\"\\nFinished processing.......\")\n",
        "\n",
        "\n",
        "if opt.plot:\n",
        "    print('\\n Looking at some sample images')\n",
        "    plot_random_spec_img(TRAIN_PATCH, TRAIN_LABELS)\n",
        "    plot_random_spec_img(TEST_PATCH, TEST_LABELS)\n",
        "    plot_random_spec_img(VAL_PATCH, VAL_LABELS)\n",
        "\n",
        "    GroundTruthVisualise(target_mat)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fw2Suc7V8BLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "outputId": "99231a21-fa0a-4e34-e6f5-546deda1a055",
        "id": "dCbP83F17LK7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: ipykernel_launcher.py [-h] [--data DATA] [--train_ratio TRAIN_RATIO]\n",
            "                             [--validation_ratio VALIDATION_RATIO]\n",
            "                             [--channel_first CHANNEL_FIRST] [--dtype DTYPE]\n",
            "                             [--plot PLOT]\n",
            "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-574540a8-69a1-4d7b-baa4-d88daa92bfc0.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from random import shuffle\n",
        "import scipy.io as io\n",
        "import argparse\n",
        "from helper import *\n",
        "import threading\n",
        "import time\n",
        "import itertools\n",
        "import sys\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--data', type=str, default='Indian_pines', help='Default: Indian_pines, options: Salinas, KSC, Botswana')\n",
        "parser.add_argument('--train_ratio', type=float, default=0.2)\n",
        "parser.add_argument('--validation_ratio', type=float, default=0.05)\n",
        "parser.add_argument('--channel_first', type=bool, default=False, help='Image channel located on the last dimension')\n",
        "parser.add_argument('--dtype', type=str, default='float32', help='Data type (Eg float64, float32, float16, int64...')\n",
        "parser.add_argument('--plot', type=bool, default=False, help='TRUE to plot satellite images and ground truth at the end')\n",
        "opt = parser.parse_args()\n",
        "\n",
        "# Try loading data from the folder... Otherwise download from online\n",
        "input_mat, target_mat = maybeDownloadOrExtract(opt.data)\n",
        "\n",
        "# Output data type\n",
        "datatype = getdtype(opt.dtype)\n",
        "HEIGHT = input_mat.shape[0]\n",
        "WIDTH = input_mat.shape[1]\n",
        "BAND = input_mat.shape[2]\n",
        "OUTPUT_CLASSES = np.max(target_mat)\n",
        "PATCH_SIZE = 5\n",
        "\n",
        "CHANNEL_FIRST = opt.channel_first\n",
        "\n",
        "# Normalize image data and select datatype\n",
        "input_mat = input_mat.astype(datatype)\n",
        "input_mat = input_mat - np.min(input_mat)\n",
        "input_mat = input_mat / np.max(input_mat)\n",
        "\n",
        "# Extract a list that contains the class number with sufficient training samples\n",
        "list_labels = getListLabel(opt.data)\n",
        "\n",
        "# For showing a animation only\n",
        "end_loading = False\n",
        "def animate():\n",
        "    global end_loading\n",
        "    for c in itertools.cycle(['|', '/', '-', '\\\\']):\n",
        "        if end_loading:\n",
        "            break\n",
        "        sys.stdout.write('\\rExtracting '+ opt.data + ' dataset features...' + c)\n",
        "        sys.stdout.flush()\n",
        "        time.sleep(0.1)\n",
        "        sys.stdout.write('\\rFinished!\\t')\n",
        "\n",
        "print(\"+-------------------------------------+\")\n",
        "print('Input_mat shape: ' + str(input_mat.shape))\n",
        "\n",
        "MEAN_ARRAY = np.ndarray(shape=(BAND, 1))\n",
        "new_input_mat = []\n",
        "\n",
        "input_mat = np.transpose(input_mat, (2, 0, 1))\n",
        "\n",
        "calib_val_pad = int((PATCH_SIZE - 1)/2)\n",
        "for i in range(BAND):\n",
        "    MEAN_ARRAY[i] = np.mean(input_mat[i, :, :])\n",
        "    new_input_mat.append(np.pad(input_mat[i, :, :], calib_val_pad, 'constant', constant_values=0))\n",
        "\n",
        "input_mat = np.array(new_input_mat)\n",
        "\n",
        "def Patch(height_index, width_index):\n",
        "\n",
        "    # Input:\n",
        "    # Given the index position (x,y) of spatio dimension of the hyperspectral image,\n",
        "\n",
        "    # Output:\n",
        "    # a data cube with patch size S (24 neighbours), with label based on central pixel\n",
        "\n",
        "    height_slice = slice(height_index, height_index+PATCH_SIZE)\n",
        "    width_slice = slice(width_index, width_index+PATCH_SIZE)\n",
        "\n",
        "    patch = input_mat[:, height_slice, width_slice]\n",
        "    mean_normalized_patch = []\n",
        "    for i in range(patch.shape[0]):\n",
        "        mean_normalized_patch.append(patch[i] - MEAN_ARRAY[i])\n",
        "\n",
        "    return np.array(mean_normalized_patch).astype(datatype)\n",
        "\n",
        "\n",
        "# Assign empty array to store patched images\n",
        "CLASSES = []\n",
        "for i in range(OUTPUT_CLASSES):\n",
        "    CLASSES.append([])\n",
        "\n",
        "# Assign empty array to count samples in each class\n",
        "class_label_counter = [0] * OUTPUT_CLASSES\n",
        "\n",
        "# Start timing for loading\n",
        "t = threading.Thread(target=animate).start()\n",
        "start = time.time()\n",
        "\n",
        "count = 0\n",
        "for i in range(HEIGHT-1):\n",
        "    for j in range(WIDTH-1):\n",
        "        curr_inp = Patch(i, j)\n",
        "        curr_tar = target_mat[i, j]\n",
        "\n",
        "        if curr_tar:\n",
        "            CLASSES[curr_tar-1].append(curr_inp)\n",
        "            class_label_counter[curr_tar-1] += 1\n",
        "            count += 1\n",
        "\n",
        "end_loading = True\n",
        "end = time.time()\n",
        "print(\"Total excution time...\" + str(end-start)+'seconds')\n",
        "print('Total number of samples: ' + str(count))\n",
        "showClassTable(class_label_counter)\n",
        "\n",
        "TRAIN_PATCH, TRAIN_LABELS = [], []\n",
        "TEST_PATCH, TEST_LABELS =[], []\n",
        "VAL_PATCH, VAL_LABELS = [], []\n",
        "\n",
        "train_ratio = opt.train_ratio\n",
        "val_ratio = opt.validation_ratio\n",
        "# test_ratio = reminder of data\n",
        "\n",
        "counter = 0  # Represent train_index position\n",
        "for i, data in enumerate(CLASSES):\n",
        "    datasize = []\n",
        "    if i + 1 in list_labels:\n",
        "\n",
        "        shuffle(data)\n",
        "        print('Class ' + str(i + 1) + ' is accepted')\n",
        "\n",
        "        size = round(class_label_counter[i]*train_ratio)\n",
        "\n",
        "        TRAIN_PATCH += data[:size]\n",
        "        TRAIN_LABELS += [counter] * size\n",
        "        datasize.append(size)\n",
        "\n",
        "        size1 = round(class_label_counter[i]*val_ratio)\n",
        "        VAL_PATCH += data[size:size+size1]\n",
        "        VAL_LABELS += [counter] * (size1)\n",
        "        datasize.append(size1)\n",
        "\n",
        "        TEST_PATCH += data[size+size1:]\n",
        "        TEST_LABELS += [counter] * len(data[size+size1:])\n",
        "        datasize.append(len(TEST_PATCH))\n",
        "\n",
        "        counter += 1\n",
        "\n",
        "    else:\n",
        "        print('-Class ' + str(i + 1) + ' is rejected due to insufficient samples')\n",
        "\n",
        "TRAIN_LABELS = np.array(TRAIN_LABELS)\n",
        "TRAIN_PATCH = np.array(TRAIN_PATCH)\n",
        "TEST_PATCH = np.array(TEST_PATCH)\n",
        "TEST_LABELS = np.array(TEST_LABELS)\n",
        "VAL_PATCH = np.array(VAL_PATCH)\n",
        "VAL_LABELS = np.array(VAL_LABELS)\n",
        "\n",
        "print(\"+-------------------------------------+\")\n",
        "print(\"Size of Training data: \" + str(len(TRAIN_PATCH)) )\n",
        "print(\"Size of Validation data: \" + str(len(VAL_PATCH))  )\n",
        "print(\"Size of Testing data: \" + str(len(TEST_PATCH)) )\n",
        "print(\"+-------------------------------------+\")\n",
        "\n",
        "\n",
        "train_idx = list(range(len(TRAIN_PATCH)))\n",
        "shuffle(train_idx)\n",
        "TRAIN_PATCH = TRAIN_PATCH[train_idx]\n",
        "if not CHANNEL_FIRST:\n",
        "    TRAIN_PATCH = np.transpose(TRAIN_PATCH, (0, 2, 3, 1))\n",
        "TRAIN_LABELS = OnehotTransform(TRAIN_LABELS[train_idx])\n",
        "train = {}\n",
        "train[\"train_patch\"] = TRAIN_PATCH\n",
        "train[\"train_labels\"] = TRAIN_LABELS\n",
        "io.savemat(\"./data/\" + opt.data + \"_Train_patch_\" + str(PATCH_SIZE) + \".mat\", train)\n",
        "\n",
        "\n",
        "test_idx = list(range(len(TEST_PATCH)))\n",
        "shuffle(test_idx)\n",
        "TEST_PATCH = TEST_PATCH[test_idx]\n",
        "if not CHANNEL_FIRST:\n",
        "    TEST_PATCH = np.transpose(TEST_PATCH, (0, 2, 3, 1))\n",
        "TEST_LABELS = OnehotTransform(TEST_LABELS[test_idx])\n",
        "test = {}\n",
        "test[\"test_patch\"] = TEST_PATCH\n",
        "test[\"test_labels\"] = TEST_LABELS\n",
        "io.savemat(\"./data/\" + opt.data + \"_Test_patch_\" + str(PATCH_SIZE) + \".mat\", test)\n",
        "\n",
        "\n",
        "val_idx = list(range(len(VAL_PATCH)))\n",
        "shuffle(val_idx)\n",
        "VAL_PATCH = VAL_PATCH[val_idx]\n",
        "if not CHANNEL_FIRST:\n",
        "    VAL_PATCH = np.transpose(VAL_PATCH, (0, 2, 3, 1))\n",
        "    print(VAL_PATCH.shape)\n",
        "VAL_LABELS = OnehotTransform(VAL_LABELS[val_idx])\n",
        "val = {}\n",
        "val[\"val_patch\"] = VAL_PATCH\n",
        "val[\"val_labels\"] = VAL_LABELS\n",
        "io.savemat(\"./data/\" + opt.data + \"_Val_patch_\" + str(PATCH_SIZE) + \".mat\", val)\n",
        "\n",
        "print(\"+-------------------------------------+\")\n",
        "print(\"Summary\")\n",
        "print('Train_patch.shape: '+ str(TRAIN_PATCH.shape) )\n",
        "print('Train_label.shape: '+ str(TRAIN_LABELS.shape) )\n",
        "print('Test_patch.shape: ' + str(TEST_PATCH.shape))\n",
        "print('Test_label.shape: ' + str(TEST_LABELS.shape))\n",
        "print(\"Validation batch Shape: \" + str(VAL_PATCH.shape) )\n",
        "print(\"Validation label Shape: \" + str(VAL_LABELS.shape) )\n",
        "print(\"+-------------------------------------+\")\n",
        "print(\"\\nFinished processing.......\")\n",
        "\n",
        "\n",
        "if opt.plot:\n",
        "    print('\\n Looking at some sample images')\n",
        "    plot_random_spec_img(TRAIN_PATCH, TRAIN_LABELS)\n",
        "    plot_random_spec_img(TEST_PATCH, TEST_LABELS)\n",
        "    plot_random_spec_img(VAL_PATCH, VAL_LABELS)\n",
        "\n",
        "    GroundTruthVisualise(target_mat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "outputId": "99231a21-fa0a-4e34-e6f5-546deda1a055",
        "id": "L9Cv6n6S7Mr2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: ipykernel_launcher.py [-h] [--data DATA] [--train_ratio TRAIN_RATIO]\n",
            "                             [--validation_ratio VALIDATION_RATIO]\n",
            "                             [--channel_first CHANNEL_FIRST] [--dtype DTYPE]\n",
            "                             [--plot PLOT]\n",
            "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-574540a8-69a1-4d7b-baa4-d88daa92bfc0.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from random import shuffle\n",
        "import scipy.io as io\n",
        "import argparse\n",
        "from helper import *\n",
        "import threading\n",
        "import time\n",
        "import itertools\n",
        "import sys\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--data', type=str, default='Indian_pines', help='Default: Indian_pines, options: Salinas, KSC, Botswana')\n",
        "parser.add_argument('--train_ratio', type=float, default=0.2)\n",
        "parser.add_argument('--validation_ratio', type=float, default=0.05)\n",
        "parser.add_argument('--channel_first', type=bool, default=False, help='Image channel located on the last dimension')\n",
        "parser.add_argument('--dtype', type=str, default='float32', help='Data type (Eg float64, float32, float16, int64...')\n",
        "parser.add_argument('--plot', type=bool, default=False, help='TRUE to plot satellite images and ground truth at the end')\n",
        "opt = parser.parse_args()\n",
        "\n",
        "# Try loading data from the folder... Otherwise download from online\n",
        "input_mat, target_mat = maybeDownloadOrExtract(opt.data)\n",
        "\n",
        "# Output data type\n",
        "datatype = getdtype(opt.dtype)\n",
        "HEIGHT = input_mat.shape[0]\n",
        "WIDTH = input_mat.shape[1]\n",
        "BAND = input_mat.shape[2]\n",
        "OUTPUT_CLASSES = np.max(target_mat)\n",
        "PATCH_SIZE = 5\n",
        "\n",
        "CHANNEL_FIRST = opt.channel_first\n",
        "\n",
        "# Normalize image data and select datatype\n",
        "input_mat = input_mat.astype(datatype)\n",
        "input_mat = input_mat - np.min(input_mat)\n",
        "input_mat = input_mat / np.max(input_mat)\n",
        "\n",
        "# Extract a list that contains the class number with sufficient training samples\n",
        "list_labels = getListLabel(opt.data)\n",
        "\n",
        "# For showing a animation only\n",
        "end_loading = False\n",
        "def animate():\n",
        "    global end_loading\n",
        "    for c in itertools.cycle(['|', '/', '-', '\\\\']):\n",
        "        if end_loading:\n",
        "            break\n",
        "        sys.stdout.write('\\rExtracting '+ opt.data + ' dataset features...' + c)\n",
        "        sys.stdout.flush()\n",
        "        time.sleep(0.1)\n",
        "        sys.stdout.write('\\rFinished!\\t')\n",
        "\n",
        "print(\"+-------------------------------------+\")\n",
        "print('Input_mat shape: ' + str(input_mat.shape))\n",
        "\n",
        "MEAN_ARRAY = np.ndarray(shape=(BAND, 1))\n",
        "new_input_mat = []\n",
        "\n",
        "input_mat = np.transpose(input_mat, (2, 0, 1))\n",
        "\n",
        "calib_val_pad = int((PATCH_SIZE - 1)/2)\n",
        "for i in range(BAND):\n",
        "    MEAN_ARRAY[i] = np.mean(input_mat[i, :, :])\n",
        "    new_input_mat.append(np.pad(input_mat[i, :, :], calib_val_pad, 'constant', constant_values=0))\n",
        "\n",
        "input_mat = np.array(new_input_mat)\n",
        "\n",
        "def Patch(height_index, width_index):\n",
        "\n",
        "    # Input:\n",
        "    # Given the index position (x,y) of spatio dimension of the hyperspectral image,\n",
        "\n",
        "    # Output:\n",
        "    # a data cube with patch size S (24 neighbours), with label based on central pixel\n",
        "\n",
        "    height_slice = slice(height_index, height_index+PATCH_SIZE)\n",
        "    width_slice = slice(width_index, width_index+PATCH_SIZE)\n",
        "\n",
        "    patch = input_mat[:, height_slice, width_slice]\n",
        "    mean_normalized_patch = []\n",
        "    for i in range(patch.shape[0]):\n",
        "        mean_normalized_patch.append(patch[i] - MEAN_ARRAY[i])\n",
        "\n",
        "    return np.array(mean_normalized_patch).astype(datatype)\n",
        "\n",
        "\n",
        "# Assign empty array to store patched images\n",
        "CLASSES = []\n",
        "for i in range(OUTPUT_CLASSES):\n",
        "    CLASSES.append([])\n",
        "\n",
        "# Assign empty array to count samples in each class\n",
        "class_label_counter = [0] * OUTPUT_CLASSES\n",
        "\n",
        "# Start timing for loading\n",
        "t = threading.Thread(target=animate).start()\n",
        "start = time.time()\n",
        "\n",
        "count = 0\n",
        "for i in range(HEIGHT-1):\n",
        "    for j in range(WIDTH-1):\n",
        "        curr_inp = Patch(i, j)\n",
        "        curr_tar = target_mat[i, j]\n",
        "\n",
        "        if curr_tar:\n",
        "            CLASSES[curr_tar-1].append(curr_inp)\n",
        "            class_label_counter[curr_tar-1] += 1\n",
        "            count += 1\n",
        "\n",
        "end_loading = True\n",
        "end = time.time()\n",
        "print(\"Total excution time...\" + str(end-start)+'seconds')\n",
        "print('Total number of samples: ' + str(count))\n",
        "showClassTable(class_label_counter)\n",
        "\n",
        "TRAIN_PATCH, TRAIN_LABELS = [], []\n",
        "TEST_PATCH, TEST_LABELS =[], []\n",
        "VAL_PATCH, VAL_LABELS = [], []\n",
        "\n",
        "train_ratio = opt.train_ratio\n",
        "val_ratio = opt.validation_ratio\n",
        "# test_ratio = reminder of data\n",
        "\n",
        "counter = 0  # Represent train_index position\n",
        "for i, data in enumerate(CLASSES):\n",
        "    datasize = []\n",
        "    if i + 1 in list_labels:\n",
        "\n",
        "        shuffle(data)\n",
        "        print('Class ' + str(i + 1) + ' is accepted')\n",
        "\n",
        "        size = round(class_label_counter[i]*train_ratio)\n",
        "\n",
        "        TRAIN_PATCH += data[:size]\n",
        "        TRAIN_LABELS += [counter] * size\n",
        "        datasize.append(size)\n",
        "\n",
        "        size1 = round(class_label_counter[i]*val_ratio)\n",
        "        VAL_PATCH += data[size:size+size1]\n",
        "        VAL_LABELS += [counter] * (size1)\n",
        "        datasize.append(size1)\n",
        "\n",
        "        TEST_PATCH += data[size+size1:]\n",
        "        TEST_LABELS += [counter] * len(data[size+size1:])\n",
        "        datasize.append(len(TEST_PATCH))\n",
        "\n",
        "        counter += 1\n",
        "\n",
        "    else:\n",
        "        print('-Class ' + str(i + 1) + ' is rejected due to insufficient samples')\n",
        "\n",
        "TRAIN_LABELS = np.array(TRAIN_LABELS)\n",
        "TRAIN_PATCH = np.array(TRAIN_PATCH)\n",
        "TEST_PATCH = np.array(TEST_PATCH)\n",
        "TEST_LABELS = np.array(TEST_LABELS)\n",
        "VAL_PATCH = np.array(VAL_PATCH)\n",
        "VAL_LABELS = np.array(VAL_LABELS)\n",
        "\n",
        "print(\"+-------------------------------------+\")\n",
        "print(\"Size of Training data: \" + str(len(TRAIN_PATCH)) )\n",
        "print(\"Size of Validation data: \" + str(len(VAL_PATCH))  )\n",
        "print(\"Size of Testing data: \" + str(len(TEST_PATCH)) )\n",
        "print(\"+-------------------------------------+\")\n",
        "\n",
        "\n",
        "train_idx = list(range(len(TRAIN_PATCH)))\n",
        "shuffle(train_idx)\n",
        "TRAIN_PATCH = TRAIN_PATCH[train_idx]\n",
        "if not CHANNEL_FIRST:\n",
        "    TRAIN_PATCH = np.transpose(TRAIN_PATCH, (0, 2, 3, 1))\n",
        "TRAIN_LABELS = OnehotTransform(TRAIN_LABELS[train_idx])\n",
        "train = {}\n",
        "train[\"train_patch\"] = TRAIN_PATCH\n",
        "train[\"train_labels\"] = TRAIN_LABELS\n",
        "io.savemat(\"./data/\" + opt.data + \"_Train_patch_\" + str(PATCH_SIZE) + \".mat\", train)\n",
        "\n",
        "\n",
        "test_idx = list(range(len(TEST_PATCH)))\n",
        "shuffle(test_idx)\n",
        "TEST_PATCH = TEST_PATCH[test_idx]\n",
        "if not CHANNEL_FIRST:\n",
        "    TEST_PATCH = np.transpose(TEST_PATCH, (0, 2, 3, 1))\n",
        "TEST_LABELS = OnehotTransform(TEST_LABELS[test_idx])\n",
        "test = {}\n",
        "test[\"test_patch\"] = TEST_PATCH\n",
        "test[\"test_labels\"] = TEST_LABELS\n",
        "io.savemat(\"./data/\" + opt.data + \"_Test_patch_\" + str(PATCH_SIZE) + \".mat\", test)\n",
        "\n",
        "\n",
        "val_idx = list(range(len(VAL_PATCH)))\n",
        "shuffle(val_idx)\n",
        "VAL_PATCH = VAL_PATCH[val_idx]\n",
        "if not CHANNEL_FIRST:\n",
        "    VAL_PATCH = np.transpose(VAL_PATCH, (0, 2, 3, 1))\n",
        "    print(VAL_PATCH.shape)\n",
        "VAL_LABELS = OnehotTransform(VAL_LABELS[val_idx])\n",
        "val = {}\n",
        "val[\"val_patch\"] = VAL_PATCH\n",
        "val[\"val_labels\"] = VAL_LABELS\n",
        "io.savemat(\"./data/\" + opt.data + \"_Val_patch_\" + str(PATCH_SIZE) + \".mat\", val)\n",
        "\n",
        "print(\"+-------------------------------------+\")\n",
        "print(\"Summary\")\n",
        "print('Train_patch.shape: '+ str(TRAIN_PATCH.shape) )\n",
        "print('Train_label.shape: '+ str(TRAIN_LABELS.shape) )\n",
        "print('Test_patch.shape: ' + str(TEST_PATCH.shape))\n",
        "print('Test_label.shape: ' + str(TEST_LABELS.shape))\n",
        "print(\"Validation batch Shape: \" + str(VAL_PATCH.shape) )\n",
        "print(\"Validation label Shape: \" + str(VAL_LABELS.shape) )\n",
        "print(\"+-------------------------------------+\")\n",
        "print(\"\\nFinished processing.......\")\n",
        "\n",
        "\n",
        "if opt.plot:\n",
        "    print('\\n Looking at some sample images')\n",
        "    plot_random_spec_img(TRAIN_PATCH, TRAIN_LABELS)\n",
        "    plot_random_spec_img(TEST_PATCH, TEST_LABELS)\n",
        "    plot_random_spec_img(VAL_PATCH, VAL_LABELS)\n",
        "\n",
        "    GroundTruthVisualise(target_mat)"
      ]
    }
  ]
}